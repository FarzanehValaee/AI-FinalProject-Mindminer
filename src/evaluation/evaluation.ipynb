{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "04d1209d",
      "metadata": {},
      "source": [
        "# ارزیابی سیستم توصیه‌گر فیلم (Mindminer)\n",
        "\n",
        "این نوتبوک معیارهای ارزیابی مناسب برای سیستم توصیه **محتوا-محور** را محاسبه می‌کند:\n",
        "\n",
        "- **Precision@K** و **Recall@K**: دقت و بازخوانی در K توصیه اول (با تعریف مرتبط بودن بر اساس اشتراک تگ‌ها)\n",
        "- **MRR** (Mean Reciprocal Rank): رتبه معکوس اولین آیتم مرتبط\n",
        "- **NDCG@K**: Normalized Discounted Cumulative Gain\n",
        "- **پوشش کاتالوگ (Catalog Coverage)**: درصد فیلم‌هایی که حداقل یک بار در لیست توصیه‌ها ظاهر می‌شوند\n",
        "- **تنوع درون‌لیست (Intra-list Diversity)**: تنوع بین فیلم‌های توصیه‌شده"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f0358ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Project path (run from repo root or from evaluation folder)\n",
        "PROJECT_ROOT = Path(\".\").resolve()\n",
        "if \"src\" in str(PROJECT_ROOT):\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent.parent\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "from src.evaluation.metrics import (\n",
        "    precision_at_k,\n",
        "    recall_at_k,\n",
        "    mean_reciprocal_rank,\n",
        "    ndcg_at_k,\n",
        "    catalog_coverage,\n",
        "    average_diversity,\n",
        "    relevance_by_tag_overlap,\n",
        "    relevance_score_tag_overlap,\n",
        ")\n",
        "\n",
        "DATA_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"movies_merge.csv\"\n",
        "print(\"Project root:\", PROJECT_ROOT)\n",
        "print(\"Data exists:\", DATA_PATH.exists())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a970895a",
      "metadata": {},
      "source": [
        "## Load Data and Build Model (same pipeline as training)\n",
        "\n",
        "Data is loaded, stemming is applied to the `tags` column, and vectors are built with CountVectorizer and cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf40b13",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Columns:\", list(df.columns))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "729ac722",
      "metadata": {},
      "outputs": [],
      "source": [
        "ps = PorterStemmer()\n",
        "def stem(text):\n",
        "    return \" \".join(ps.stem(w) for w in (text or \"\").split())\n",
        "\n",
        "df = df.copy()\n",
        "df[\"tags\"] = df[\"tags\"].apply(stem)\n",
        "cv = CountVectorizer(max_features=5000, stop_words=\"english\")\n",
        "vectors = cv.fit_transform(df[\"tags\"]).toarray()\n",
        "similarity = cosine_similarity(vectors)\n",
        "print(\"Similarity matrix shape:\", similarity.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8dc4b27",
      "metadata": {},
      "source": [
        "## تعریف «مرتبط» و حلقه ارزیابی\n",
        "\n",
        "برای سیستم محتوا-محور بدون امتیاز کاربر، **مرتبط بودن** را با **اشتراک تگ** تعریف می‌کنیم: فیلمی مرتبط است که حداقل `min_common` تگ با فیلم پرس‌وجو داشته باشد.  \n",
        "روی یک نمونه از فیلم‌ها (یا همه) ارزیابی انجام می‌شود و برای هر کدام Precision@K، Recall@K، MRR و NDCG@K محاسبه می‌شود."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "050a2335",
      "metadata": {},
      "outputs": [],
      "source": [
        "TOP_K = 5\n",
        "MIN_COMMON_TAGS = 3   # Minimum shared tags for \"relevant\"\n",
        "SAMPLE_SIZE = 500     # Number of movies to evaluate (None = all)\n",
        "np.random.seed(42)\n",
        "n_movies = len(df)\n",
        "eval_indices = np.random.choice(n_movies, size=min(SAMPLE_SIZE, n_movies), replace=False) if SAMPLE_SIZE else np.arange(n_movies)\n",
        "print(f\"Evaluating on {len(eval_indices)} movies, top_k={TOP_K}, min_common_tags={MIN_COMMON_TAGS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db400421",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_top_k_indices(sim_row, query_idx, k=TOP_K):\n",
        "    \"\"\"Return top-k indices (excluding the query movie itself).\"\"\"\n",
        "    ranked = np.argsort(sim_row)[::-1]\n",
        "    ranked = ranked[ranked != query_idx]\n",
        "    return ranked[:k].tolist()\n",
        "\n",
        "def get_relevant_indices(query_idx, min_common=MIN_COMMON_TAGS):\n",
        "    \"\"\"List of indices of movies that share at least min_common tags with the query (excluding itself).\"\"\"\n",
        "    query_tags = df.iloc[query_idx][\"tags\"]\n",
        "    relevant = []\n",
        "    for i in range(len(df)):\n",
        "        if i == query_idx:\n",
        "            continue\n",
        "        if relevance_by_tag_overlap(query_tags, df.iloc[i][\"tags\"], min_common=min_common):\n",
        "            relevant.append(i)\n",
        "    return relevant\n",
        "\n",
        "def relevance_fn_for_ndcg(query_idx):\n",
        "    \"\"\"Relevance function for NDCG: score based on Jaccard similarity of tags.\"\"\"\n",
        "    query_tags = df.iloc[query_idx][\"tags\"]\n",
        "    def fn(candidate_idx):\n",
        "        return relevance_score_tag_overlap(query_tags, df.iloc[candidate_idx][\"tags\"])\n",
        "    return fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "911336be",
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "all_recommendations = []\n",
        "\n",
        "for idx in eval_indices:\n",
        "    sim_row = similarity[idx]\n",
        "    rec = get_top_k_indices(sim_row, idx, k=TOP_K)\n",
        "    relevant = get_relevant_indices(idx, min_common=MIN_COMMON_TAGS)\n",
        "    all_recommendations.append(rec)\n",
        "\n",
        "    # NDCG needs relevance for full ranking; we use full similarity-ordered list (excluding query)\n",
        "    rel_fn = relevance_fn_for_ndcg(idx)\n",
        "    full_ranking = np.argsort(sim_row)[::-1]\n",
        "    full_ranking = full_ranking[full_ranking != idx].tolist()\n",
        "    ndcg = ndcg_at_k(full_ranking, rel_fn, k=TOP_K)\n",
        "\n",
        "    results.append({\n",
        "        \"precision_at_k\": precision_at_k(rec, relevant, k=TOP_K),\n",
        "        \"recall_at_k\": recall_at_k(rec, relevant, k=TOP_K),\n",
        "        \"mrr\": mean_reciprocal_rank(rec, relevant),\n",
        "        \"ndcg_at_k\": ndcg,\n",
        "        \"n_relevant\": len(relevant),\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e119c349",
      "metadata": {},
      "source": [
        "## Evaluation Metrics Summary (mean over sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "598758a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "summary = {\n",
        "    \"Precision@5\": results_df[\"precision_at_k\"].mean(),\n",
        "    \"Recall@5\": results_df[\"recall_at_k\"].mean(),\n",
        "    \"MRR\": results_df[\"mrr\"].mean(),\n",
        "    \"NDCG@5\": results_df[\"ndcg_at_k\"].mean(),\n",
        "}\n",
        "summary_series = pd.Series(summary)\n",
        "print(\"Ranking and relevance metrics:\")\n",
        "display(summary_series.to_frame(\"Value\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e5a21f0",
      "metadata": {},
      "source": [
        "## Catalog Coverage and Diversity\n",
        "\n",
        "- **Catalog Coverage**: Fraction of all movies that appear in at least one top-5 recommendation list.\n",
        "- **Average Intra-list Diversity**: Mean diversity within each recommendation list (1 minus average pairwise cosine similarity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba4ae332",
      "metadata": {},
      "outputs": [],
      "source": [
        "coverage = catalog_coverage(all_recommendations, catalog_size=len(df))\n",
        "diversity = average_diversity(similarity, all_recommendations)\n",
        "print(f\"Catalog Coverage: {coverage:.4f} ({coverage*100:.2f}%)\")\n",
        "print(f\"Average Intra-list Diversity: {diversity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c499151",
      "metadata": {},
      "source": [
        "## Metrics Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ba651dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# Main metrics\n",
        "metrics_for_chart = {**summary, \"Coverage\": coverage, \"Diversity\": diversity}\n",
        "axes[0].bar(metrics_for_chart.keys(), metrics_for_chart.values(), color=[\"#2ecc71\", \"#3498db\", \"#9b59b6\", \"#e74c3c\", \"#1abc9c\", \"#f39c12\"])\n",
        "axes[0].set_ylabel(\"Value\")\n",
        "axes[0].set_title(\"Recommender System Evaluation Metrics\")\n",
        "axes[0].tick_params(axis=\"x\", rotation=25)\n",
        "\n",
        "# Precision@K distribution\n",
        "axes[1].hist(results_df[\"precision_at_k\"], bins=15, edgecolor=\"black\", alpha=0.7)\n",
        "axes[1].set_xlabel(\"Precision@5\")\n",
        "axes[1].set_ylabel(\"Count\")\n",
        "axes[1].set_title(\"Precision@5 Distribution over Sample\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bba3251",
      "metadata": {},
      "source": [
        "## Metrics Summary Table\n",
        "\n",
        "| Metric | Description |\n",
        "|--------|-------------|\n",
        "| **Precision@K** | Of the top-K recommendations, how many are \"relevant\" (tag overlap ≥ threshold). |\n",
        "| **Recall@K** | Of all relevant movies, how many appear in the top-K recommendations. |\n",
        "| **MRR** | Reciprocal rank of the first relevant movie; higher is better. |\n",
        "| **NDCG@K** | Ranking quality accounting for degree of relevance (Jaccard on tags). |\n",
        "| **Catalog Coverage** | Fraction of the catalog recommended at least once; higher means more variety. |\n",
        "| **Intra-list Diversity** | Diversity within each recommendation list; higher = more diverse recommendations. |\n",
        "\n",
        "To change the evaluation sample size, set `SAMPLE_SIZE` in the parameters cell; use `None` to evaluate on all movies."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
